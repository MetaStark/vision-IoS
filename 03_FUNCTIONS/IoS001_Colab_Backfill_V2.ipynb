{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IoS-001 QUARANTINED Asset Backfill V2\n",
    "\n",
    "**FjordHQ - STIG-2025-001 Compliant**\n",
    "\n",
    "Denne notebook henter prisdata for **kun QUARANTINED assets** (45 stk).\n",
    "\n",
    "## Korrekt Schema\n",
    "- UNIQUE: `(listing_id, date, resolution)`\n",
    "- Idempotent: `ON CONFLICT DO UPDATE`\n",
    "\n",
    "## QUARANTINED Assets:\n",
    "- **ETFs (15):** DIA, IWM, VOO, VTI, XLB-XLY\n",
    "- **Mag7 (4):** AMZN, GOOGL, META, TSLA\n",
    "- **US Large Cap (7):** JNJ, MA, V, UNH, BRK.B, HES, SQ\n",
    "- **Oslo Børs (12):** BELCO, CRAYN, FLNG, GOGL, etc.\n",
    "- **Andre (7):** ANSS, ASML, GOOG, PARA, SPLK, BT.A.L, STM.PA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install yfinance pandas tqdm -q\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!mkdir -p \"/content/drive/MyDrive/FjordHQ/ios001_backfill/data\"\n",
    "\n",
    "print(\"Setup fullført!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Konfigurasjon og Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timedelta, date\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Konfigurasjon\n",
    "CHECKPOINT_DIR = \"/content/drive/MyDrive/FjordHQ/ios001_backfill\"\n",
    "\n",
    "# Rate Limits (Colab-optimalisert)\n",
    "BATCH_SIZE = 5\n",
    "DELAY_BETWEEN_ASSETS = 8.0\n",
    "DELAY_BETWEEN_BATCHES = 180.0\n",
    "MAX_RETRIES = 5\n",
    "RETRY_BASE_DELAY = 30.0\n",
    "RATE_LIMIT_BACKOFF = 600.0\n",
    "\n",
    "# Historie\n",
    "MAX_HISTORY_YEARS = 10\n",
    "\n",
    "# Iron Curtain (IoS-001 §4.1)\n",
    "EQUITY_FX_QUARANTINE = 252\n",
    "EQUITY_FX_FULL_HISTORY = 1260\n",
    "\n",
    "print(f\"Checkpoint: {CHECKPOINT_DIR}\")\n",
    "print(f\"Rate: {DELAY_BETWEEN_ASSETS}s mellom assets, {DELAY_BETWEEN_BATCHES}s mellom batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. QUARANTINED Assets (45 stk fra database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kun assets som er QUARANTINED i fhq_meta.assets\n",
    "QUARANTINED_ASSETS = {\n",
    "    # ARCX ETFs (15)\n",
    "    \"DIA\": {\"exchange\": \"ARCX\", \"yf_ticker\": \"DIA\"},\n",
    "    \"IWM\": {\"exchange\": \"ARCX\", \"yf_ticker\": \"IWM\"},\n",
    "    \"VOO\": {\"exchange\": \"ARCX\", \"yf_ticker\": \"VOO\"},\n",
    "    \"VTI\": {\"exchange\": \"ARCX\", \"yf_ticker\": \"VTI\"},\n",
    "    \"XLB\": {\"exchange\": \"ARCX\", \"yf_ticker\": \"XLB\"},\n",
    "    \"XLC\": {\"exchange\": \"ARCX\", \"yf_ticker\": \"XLC\"},\n",
    "    \"XLE\": {\"exchange\": \"ARCX\", \"yf_ticker\": \"XLE\"},\n",
    "    \"XLF\": {\"exchange\": \"ARCX\", \"yf_ticker\": \"XLF\"},\n",
    "    \"XLI\": {\"exchange\": \"ARCX\", \"yf_ticker\": \"XLI\"},\n",
    "    \"XLK\": {\"exchange\": \"ARCX\", \"yf_ticker\": \"XLK\"},\n",
    "    \"XLP\": {\"exchange\": \"ARCX\", \"yf_ticker\": \"XLP\"},\n",
    "    \"XLRE\": {\"exchange\": \"ARCX\", \"yf_ticker\": \"XLRE\"},\n",
    "    \"XLU\": {\"exchange\": \"ARCX\", \"yf_ticker\": \"XLU\"},\n",
    "    \"XLV\": {\"exchange\": \"ARCX\", \"yf_ticker\": \"XLV\"},\n",
    "    \"XLY\": {\"exchange\": \"ARCX\", \"yf_ticker\": \"XLY\"},\n",
    "\n",
    "    # XNAS (9)\n",
    "    \"AMZN\": {\"exchange\": \"XNAS\", \"yf_ticker\": \"AMZN\"},\n",
    "    \"ANSS\": {\"exchange\": \"XNAS\", \"yf_ticker\": \"ANSS\"},\n",
    "    \"ASML\": {\"exchange\": \"XNAS\", \"yf_ticker\": \"ASML\"},\n",
    "    \"GOOG\": {\"exchange\": \"XNAS\", \"yf_ticker\": \"GOOG\"},\n",
    "    \"GOOGL\": {\"exchange\": \"XNAS\", \"yf_ticker\": \"GOOGL\"},\n",
    "    \"META\": {\"exchange\": \"XNAS\", \"yf_ticker\": \"META\"},\n",
    "    \"PARA\": {\"exchange\": \"XNAS\", \"yf_ticker\": \"PARA\"},\n",
    "    \"SPLK\": {\"exchange\": \"XNAS\", \"yf_ticker\": \"SPLK\"},\n",
    "    \"TSLA\": {\"exchange\": \"XNAS\", \"yf_ticker\": \"TSLA\"},\n",
    "\n",
    "    # XNYS (7)\n",
    "    \"BRK.B\": {\"exchange\": \"XNYS\", \"yf_ticker\": \"BRK-B\"},\n",
    "    \"HES\": {\"exchange\": \"XNYS\", \"yf_ticker\": \"HES\"},\n",
    "    \"JNJ\": {\"exchange\": \"XNYS\", \"yf_ticker\": \"JNJ\"},\n",
    "    \"MA\": {\"exchange\": \"XNYS\", \"yf_ticker\": \"MA\"},\n",
    "    \"SQ\": {\"exchange\": \"XNYS\", \"yf_ticker\": \"SQ\"},\n",
    "    \"UNH\": {\"exchange\": \"XNYS\", \"yf_ticker\": \"UNH\"},\n",
    "    \"V\": {\"exchange\": \"XNYS\", \"yf_ticker\": \"V\"},\n",
    "\n",
    "    # XOSL (12)\n",
    "    \"BELCO.OL\": {\"exchange\": \"XOSL\", \"yf_ticker\": \"BELCO.OL\"},\n",
    "    \"CRAYN.OL\": {\"exchange\": \"XOSL\", \"yf_ticker\": \"CRAYN.OL\"},\n",
    "    \"FLNG.OL\": {\"exchange\": \"XOSL\", \"yf_ticker\": \"FLNG.OL\"},\n",
    "    \"GJFAH.OL\": {\"exchange\": \"XOSL\", \"yf_ticker\": \"GJFAH.OL\"},\n",
    "    \"GOGL.OL\": {\"exchange\": \"XOSL\", \"yf_ticker\": \"GOGL.OL\"},\n",
    "    \"KAHOT.OL\": {\"exchange\": \"XOSL\", \"yf_ticker\": \"KAHOT.OL\"},\n",
    "    \"PROTCT.OL\": {\"exchange\": \"XOSL\", \"yf_ticker\": \"PROTCT.OL\"},\n",
    "    \"REC.OL\": {\"exchange\": \"XOSL\", \"yf_ticker\": \"REC.OL\"},\n",
    "    \"SCHA.OL\": {\"exchange\": \"XOSL\", \"yf_ticker\": \"SCHA.OL\"},\n",
    "    \"SCHB.OL\": {\"exchange\": \"XOSL\", \"yf_ticker\": \"SCHB.OL\"},\n",
    "    \"SRBNK.OL\": {\"exchange\": \"XOSL\", \"yf_ticker\": \"SRBNK.OL\"},\n",
    "    \"XXL.OL\": {\"exchange\": \"XOSL\", \"yf_ticker\": \"XXL.OL\"},\n",
    "\n",
    "    # XLON (1)\n",
    "    \"BT.A.L\": {\"exchange\": \"XLON\", \"yf_ticker\": \"BT-A.L\"},\n",
    "\n",
    "    # XPAR (1)\n",
    "    \"STM.PA\": {\"exchange\": \"XPAR\", \"yf_ticker\": \"STM.PA\"},\n",
    "}\n",
    "\n",
    "print(f\"QUARANTINED assets: {len(QUARANTINED_ASSETS)}\")\n",
    "print(\"\\nPer exchange:\")\n",
    "exchanges = {}\n",
    "for cid, info in QUARANTINED_ASSETS.items():\n",
    "    ex = info['exchange']\n",
    "    exchanges[ex] = exchanges.get(ex, 0) + 1\n",
    "for ex, count in sorted(exchanges.items()):\n",
    "    print(f\"  {ex}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Checkpoint System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint():\n",
    "    checkpoint_file = Path(CHECKPOINT_DIR) / \"checkpoint_v2.json\"\n",
    "    if checkpoint_file.exists():\n",
    "        with open(checkpoint_file, 'r') as f:\n",
    "            return json.load(f)\n",
    "    return {\"completed\": [], \"failed\": [], \"last_update\": None}\n",
    "\n",
    "def save_checkpoint(checkpoint):\n",
    "    checkpoint[\"last_update\"] = datetime.now().isoformat()\n",
    "    checkpoint_file = Path(CHECKPOINT_DIR) / \"checkpoint_v2.json\"\n",
    "    with open(checkpoint_file, 'w') as f:\n",
    "        json.dump(checkpoint, f, indent=2)\n",
    "\n",
    "def save_csv(canonical_id: str, df: pd.DataFrame) -> Path:\n",
    "    \"\"\"Lagre med korrekt format for fhq_data.price_series\"\"\"\n",
    "    data_dir = Path(CHECKPOINT_DIR) / \"data\"\n",
    "    data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    export_df = pd.DataFrame({\n",
    "        'listing_id': canonical_id,\n",
    "        'date': df.index,\n",
    "        'open': df['Open'],\n",
    "        'high': df['High'],\n",
    "        'low': df['Low'],\n",
    "        'close': df['Close'],\n",
    "        'adj_close': df.get('Adj Close', df['Close']),\n",
    "        'volume': df['Volume'].astype('Int64'),\n",
    "        'price_type': 'RAW',\n",
    "        'resolution': '1d',\n",
    "        'data_source': 'yfinance_colab',\n",
    "        'adr_epoch': 'COLAB_2024'\n",
    "    })\n",
    "\n",
    "    filepath = data_dir / f\"{canonical_id.replace('.', '_')}.csv\"\n",
    "    export_df.to_csv(filepath, index=False)\n",
    "    return filepath\n",
    "\n",
    "# Last eksisterende checkpoint\n",
    "checkpoint = load_checkpoint()\n",
    "print(f\"Checkpoint status:\")\n",
    "print(f\"  Fullført: {len(checkpoint['completed'])}\")\n",
    "print(f\"  Feilet: {len(checkpoint['failed'])}\")\n",
    "print(f\"  Sist: {checkpoint.get('last_update', 'Aldri')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fetch med Rate Limiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_with_backoff(yf_ticker: str, start_date, end_date) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Fetch med eksponentiell backoff for Colab\"\"\"\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            if attempt > 0:\n",
    "                delay = RETRY_BASE_DELAY * (2 ** (attempt - 1))\n",
    "                print(f\"  Retry {attempt + 1}/{MAX_RETRIES}, venter {delay:.0f}s...\")\n",
    "                time.sleep(delay)\n",
    "\n",
    "            df = yf.download(\n",
    "                yf_ticker,\n",
    "                start=start_date.strftime('%Y-%m-%d'),\n",
    "                end=(end_date + timedelta(days=1)).strftime('%Y-%m-%d'),\n",
    "                interval='1d',\n",
    "                auto_adjust=False,\n",
    "                progress=False,\n",
    "                threads=False\n",
    "            )\n",
    "\n",
    "            if df is None or df.empty:\n",
    "                continue\n",
    "\n",
    "            df = df.dropna(subset=['Close'])\n",
    "            if not df.empty:\n",
    "                return df\n",
    "\n",
    "        except Exception as e:\n",
    "            error_str = str(e).lower()\n",
    "            if any(x in error_str for x in [\"rate limit\", \"429\", \"too many\"]):\n",
    "                print(f\"  RATE LIMITED! Venter {RATE_LIMIT_BACKOFF}s...\")\n",
    "                time.sleep(RATE_LIMIT_BACKOFF)\n",
    "            else:\n",
    "                print(f\"  Feil: {e}\")\n",
    "\n",
    "    return None\n",
    "\n",
    "print(\"Fetch-funksjon klar!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Kjør Backfill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_backfill(resume=True):\n",
    "    \"\"\"Backfill alle QUARANTINED assets\"\"\"\n",
    "    global checkpoint\n",
    "\n",
    "    if not resume:\n",
    "        checkpoint = {\"completed\": [], \"failed\": [], \"last_update\": None}\n",
    "\n",
    "    pending = [cid for cid in QUARANTINED_ASSETS.keys()\n",
    "               if cid not in checkpoint[\"completed\"]]\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"IoS-001 QUARANTINED BACKFILL\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total: {len(QUARANTINED_ASSETS)}\")\n",
    "    print(f\"Fullført: {len(checkpoint['completed'])}\")\n",
    "    print(f\"Gjenstår: {len(pending)}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    if not pending:\n",
    "        print(\"Alle assets prosessert!\")\n",
    "        return\n",
    "\n",
    "    end_date = date.today() - timedelta(days=1)\n",
    "    start_date = date.today() - timedelta(days=MAX_HISTORY_YEARS * 365)\n",
    "\n",
    "    results = {\"processed\": 0, \"success\": 0, \"failed\": 0, \"total_rows\": 0}\n",
    "    batch_count = 0\n",
    "\n",
    "    for i, canonical_id in enumerate(tqdm(pending, desc=\"Backfill\")):\n",
    "        info = QUARANTINED_ASSETS[canonical_id]\n",
    "        yf_ticker = info[\"yf_ticker\"]\n",
    "\n",
    "        results[\"processed\"] += 1\n",
    "        print(f\"\\n[{i+1}/{len(pending)}] {canonical_id} ({yf_ticker})...\")\n",
    "\n",
    "        try:\n",
    "            df = fetch_with_backoff(yf_ticker, start_date, end_date)\n",
    "\n",
    "            if df is None or df.empty:\n",
    "                print(f\"  FEILET: Ingen data\")\n",
    "                results[\"failed\"] += 1\n",
    "                if canonical_id not in checkpoint[\"failed\"]:\n",
    "                    checkpoint[\"failed\"].append(canonical_id)\n",
    "            else:\n",
    "                csv_path = save_csv(canonical_id, df)\n",
    "                rows = len(df)\n",
    "                results[\"total_rows\"] += rows\n",
    "\n",
    "                if rows < EQUITY_FX_QUARANTINE:\n",
    "                    status = \"QUARANTINED\"\n",
    "                elif rows < EQUITY_FX_FULL_HISTORY:\n",
    "                    status = \"SHORT_HISTORY\"\n",
    "                else:\n",
    "                    status = \"FULL_HISTORY\"\n",
    "\n",
    "                print(f\"  OK: {rows} rader -> {status}\")\n",
    "                results[\"success\"] += 1\n",
    "\n",
    "                if canonical_id not in checkpoint[\"completed\"]:\n",
    "                    checkpoint[\"completed\"].append(canonical_id)\n",
    "                if canonical_id in checkpoint[\"failed\"]:\n",
    "                    checkpoint[\"failed\"].remove(canonical_id)\n",
    "\n",
    "            save_checkpoint(checkpoint)\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\nAvbrutt! Checkpoint lagret.\")\n",
    "            save_checkpoint(checkpoint)\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR: {e}\")\n",
    "            results[\"failed\"] += 1\n",
    "\n",
    "        batch_count += 1\n",
    "        time.sleep(DELAY_BETWEEN_ASSETS)\n",
    "\n",
    "        if batch_count >= BATCH_SIZE:\n",
    "            batch_count = 0\n",
    "            print(f\"\\nBatch pause: {DELAY_BETWEEN_BATCHES}s...\")\n",
    "            time.sleep(DELAY_BETWEEN_BATCHES)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"FULLFØRT\")\n",
    "    print(f\"Suksess: {results['success']}/{results['processed']}\")\n",
    "    print(f\"Totalt rader: {results['total_rows']}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. START BACKFILL HER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kjør backfill for alle 45 QUARANTINED assets\n",
    "# Resume=True fortsetter fra forrige checkpoint ved disconnect\n",
    "results = run_backfill(resume=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Sjekk Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oppdater og vis status\n",
    "checkpoint = load_checkpoint()\n",
    "data_dir = Path(CHECKPOINT_DIR) / \"data\"\n",
    "csv_files = list(data_dir.glob(\"*.csv\")) if data_dir.exists() else []\n",
    "\n",
    "status_counts = {\"FULL_HISTORY\": 0, \"SHORT_HISTORY\": 0, \"QUARANTINED\": 0}\n",
    "total_rows = 0\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    df = pd.read_csv(csv_file)\n",
    "    rows = len(df)\n",
    "    total_rows += rows\n",
    "\n",
    "    if rows < EQUITY_FX_QUARANTINE:\n",
    "        status_counts[\"QUARANTINED\"] += 1\n",
    "    elif rows < EQUITY_FX_FULL_HISTORY:\n",
    "        status_counts[\"SHORT_HISTORY\"] += 1\n",
    "    else:\n",
    "        status_counts[\"FULL_HISTORY\"] += 1\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"IoS-001 BACKFILL STATUS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Checkpoint: {len(checkpoint['completed'])}/{len(QUARANTINED_ASSETS)} fullført\")\n",
    "print(f\"Feilet: {len(checkpoint['failed'])}\")\n",
    "print(f\"CSV-filer: {len(csv_files)}\")\n",
    "print(f\"Totalt rader: {total_rows:,}\")\n",
    "print()\n",
    "print(\"Iron Curtain Status:\")\n",
    "print(f\"  FULL_HISTORY (5+ år): {status_counts['FULL_HISTORY']}\")\n",
    "print(f\"  SHORT_HISTORY (1-5 år): {status_counts['SHORT_HISTORY']}\")\n",
    "print(f\"  QUARANTINED (<1 år): {status_counts['QUARANTINED']}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Generer Import Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generer Python script for lokal import\n",
    "import_script = f'''#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "IoS-001 IMPORT SCRIPT - fhq_data.price_series\n",
    "Generert: {datetime.now().isoformat()}\n",
    "UNIQUE: (listing_id, date, resolution)\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from pathlib import Path\n",
    "\n",
    "DB = {{\n",
    "    \"host\": \"127.0.0.1\",\n",
    "    \"port\": \"54322\",\n",
    "    \"database\": \"postgres\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"postgres\"\n",
    "}}\n",
    "\n",
    "DATA_DIR = Path(\"./ios001_data\")\n",
    "\n",
    "def import_csv(conn, filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    sql = \"\"\"\n",
    "        INSERT INTO fhq_data.price_series (\n",
    "            listing_id, date, open, high, low, close, adj_close,\n",
    "            volume, price_type, resolution, data_source, adr_epoch\n",
    "        ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s::price_type, %s::resolution, %s, %s)\n",
    "        ON CONFLICT (listing_id, date, resolution) DO UPDATE SET\n",
    "            open = EXCLUDED.open, high = EXCLUDED.high, low = EXCLUDED.low,\n",
    "            close = EXCLUDED.close, adj_close = EXCLUDED.adj_close,\n",
    "            volume = EXCLUDED.volume, data_source = EXCLUDED.data_source\n",
    "    \"\"\"\n",
    "    inserted = 0\n",
    "    with conn.cursor() as cur:\n",
    "        for _, row in df.iterrows():\n",
    "            cur.execute(sql, (\n",
    "                row[\"listing_id\"], row[\"date\"],\n",
    "                row[\"open\"] if pd.notna(row[\"open\"]) else None,\n",
    "                row[\"high\"] if pd.notna(row[\"high\"]) else None,\n",
    "                row[\"low\"] if pd.notna(row[\"low\"]) else None,\n",
    "                row[\"close\"] if pd.notna(row[\"close\"]) else None,\n",
    "                row[\"adj_close\"] if pd.notna(row[\"adj_close\"]) else None,\n",
    "                int(row[\"volume\"]) if pd.notna(row[\"volume\"]) else None,\n",
    "                row[\"price_type\"], row[\"resolution\"],\n",
    "                row[\"data_source\"], row[\"adr_epoch\"]\n",
    "            ))\n",
    "            inserted += 1\n",
    "        conn.commit()\n",
    "    return inserted\n",
    "\n",
    "def update_status(conn, canonical_id):\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"SELECT COUNT(*) FROM fhq_data.price_series WHERE listing_id = %s AND close IS NOT NULL\", (canonical_id,))\n",
    "        count = cur.fetchone()[0]\n",
    "        status = \"QUARANTINED\" if count < 252 else \"SHORT_HISTORY\" if count < 1260 else \"FULL_HISTORY\"\n",
    "        cur.execute(\"UPDATE fhq_meta.assets SET valid_row_count = %s, data_quality_status = %s::data_quality_status WHERE canonical_id = %s\",\n",
    "                    (count, status, canonical_id))\n",
    "        conn.commit()\n",
    "    return count, status\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    conn = psycopg2.connect(**DB)\n",
    "    files = list(DATA_DIR.glob(\"*.csv\"))\n",
    "    print(f\"Importerer {{len(files)}} filer...\")\n",
    "    total = 0\n",
    "    for f in files:\n",
    "        cid = f.stem.replace(\"_\", \".\")\n",
    "        try:\n",
    "            rows = import_csv(conn, f)\n",
    "            count, status = update_status(conn, cid)\n",
    "            total += rows\n",
    "            print(f\"{{cid}}: {{rows}} -> {{status}} ({{count}} total)\")\n",
    "        except Exception as e:\n",
    "            print(f\"{{cid}}: FEIL - {{e}}\")\n",
    "    conn.close()\n",
    "    print(f\"Totalt: {{total}} rader\")\n",
    "'''\n",
    "\n",
    "script_path = Path(CHECKPOINT_DIR) / \"import_to_db_v2.py\"\n",
    "with open(script_path, 'w') as f:\n",
    "    f.write(import_script)\n",
    "\n",
    "print(f\"Import script: {script_path}\")\n",
    "print()\n",
    "print(\"INSTRUKSJONER:\")\n",
    "print(\"1. Last ned /data/ mappen fra Google Drive\")\n",
    "print(\"2. Kopier CSV-filer til ./ios001_data/\")\n",
    "print(\"3. Kopier import_to_db_v2.py\")\n",
    "print(\"4. Kjør: python import_to_db_v2.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Retry Feilede Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vis og retry feilede assets\n",
    "checkpoint = load_checkpoint()\n",
    "\n",
    "if checkpoint[\"failed\"]:\n",
    "    print(f\"Feilede assets ({len(checkpoint['failed'])}):\")\n",
    "    for cid in checkpoint[\"failed\"]:\n",
    "        print(f\"  - {cid}\")\n",
    "    print()\n",
    "    print(\"Fjern fra failed-listen for å retry:\")\n",
    "    print(\"checkpoint['failed'] = []\")\n",
    "    print(\"save_checkpoint(checkpoint)\")\n",
    "    print(\"run_backfill(resume=True)\")\n",
    "else:\n",
    "    print(\"Ingen feilede assets!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
