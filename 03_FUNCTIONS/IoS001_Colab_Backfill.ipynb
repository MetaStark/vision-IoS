{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IoS-001 Asset Backfill - FjordHQ\n",
    "\n",
    "**STIG-2025-001 Directive Compliant**\n",
    "\n",
    "Denne notebook henter prisdata for 500+ assets med:\n",
    "- Rate limiting optimalisert for Colab\n",
    "- Checkpoint/resume ved disconnect\n",
    "- Iron Curtain compliance (IoS-001 §4.1)\n",
    "- 3+ års historie for StatArb\n",
    "\n",
    "## Pools:\n",
    "- **A**: Top ETFs (SPY, QQQ, etc.)\n",
    "- **B**: Mag7 + Sector Leaders\n",
    "- **C**: Crypto Top 25\n",
    "- **D**: S&P 500 Extended\n",
    "- **E**: FX Majors\n",
    "- **F**: Oslo Børs\n",
    "- **G**: European Majors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installer dependencies\n",
    "!pip install yfinance pandas tqdm -q\n",
    "\n",
    "# Mount Google Drive for checkpoints\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Opprett checkpoint mappe\n",
    "!mkdir -p \"/content/drive/MyDrive/FjordHQ/ios001_backfill/data\"\n",
    "\n",
    "print(\"Setup fullført!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Konfigurasjon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime, timedelta, date\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Konfigurasjon\n",
    "CHECKPOINT_DIR = \"/content/drive/MyDrive/FjordHQ/ios001_backfill\"\n",
    "\n",
    "# Rate Limits (Colab-optimalisert)\n",
    "BATCH_SIZE = 5\n",
    "DELAY_BETWEEN_ASSETS = 8.0  # sekunder\n",
    "DELAY_BETWEEN_BATCHES = 180.0  # sekunder (3 min)\n",
    "MAX_RETRIES = 5\n",
    "RETRY_BASE_DELAY = 30.0\n",
    "RATE_LIMIT_BACKOFF = 600.0  # 10 min\n",
    "\n",
    "# Historie\n",
    "MAX_HISTORY_YEARS = 10\n",
    "\n",
    "# Iron Curtain Thresholds (IoS-001 §4.1)\n",
    "EQUITY_FX_QUARANTINE = 252\n",
    "EQUITY_FX_FULL_HISTORY = 1260\n",
    "CRYPTO_QUARANTINE = 365\n",
    "CRYPTO_FULL_HISTORY = 1825\n",
    "\n",
    "print(f\"Checkpoint dir: {CHECKPOINT_DIR}\")\n",
    "print(f\"Rate limits: {DELAY_BETWEEN_ASSETS}s mellom assets, {DELAY_BETWEEN_BATCHES}s mellom batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Asset Pools (STIG-2025-001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asset Universe Pools\n",
    "POOLS = {\n",
    "    \"A\": {\n",
    "        \"name\": \"Top ETFs & Index\",\n",
    "        \"tickers\": [\n",
    "            \"SPY\", \"QQQ\", \"IWM\", \"DIA\", \"XLF\", \"XLE\", \"XLK\", \"XLV\",\n",
    "            \"XLI\", \"XLU\", \"XLP\", \"XLY\", \"XLB\", \"XLRE\", \"XLC\",\n",
    "            \"VTI\", \"VOO\", \"VEA\", \"VWO\", \"BND\", \"GLD\", \"SLV\", \"USO\"\n",
    "        ]\n",
    "    },\n",
    "    \"B\": {\n",
    "        \"name\": \"Mag7 + Sector Leaders\",\n",
    "        \"tickers\": [\n",
    "            \"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\", \"NVDA\", \"META\", \"TSLA\",\n",
    "            \"JPM\", \"V\", \"MA\", \"JNJ\", \"UNH\", \"PG\", \"HD\", \"BAC\",\n",
    "            \"XOM\", \"CVX\", \"PFE\", \"ABBV\", \"KO\", \"PEP\", \"MRK\", \"TMO\",\n",
    "            \"COST\", \"WMT\", \"DIS\", \"NFLX\", \"ADBE\", \"CRM\", \"ORCL\"\n",
    "        ]\n",
    "    },\n",
    "    \"C\": {\n",
    "        \"name\": \"Crypto Top 25\",\n",
    "        \"tickers\": [\n",
    "            \"BTC-USD\", \"ETH-USD\", \"BNB-USD\", \"XRP-USD\", \"ADA-USD\",\n",
    "            \"SOL-USD\", \"DOGE-USD\", \"DOT-USD\", \"MATIC-USD\", \"SHIB-USD\",\n",
    "            \"LTC-USD\", \"TRX-USD\", \"AVAX-USD\", \"LINK-USD\", \"ATOM-USD\",\n",
    "            \"UNI-USD\", \"XMR-USD\", \"ETC-USD\", \"XLM-USD\", \"BCH-USD\",\n",
    "            \"ALGO-USD\", \"VET-USD\", \"FIL-USD\", \"ICP-USD\", \"AAVE-USD\"\n",
    "        ]\n",
    "    },\n",
    "    \"D\": {\n",
    "        \"name\": \"S&P 500 Extended\",\n",
    "        \"tickers\": [\n",
    "            \"GS\", \"MS\", \"C\", \"WFC\", \"AXP\", \"BLK\", \"SCHW\", \"CME\",\n",
    "            \"AMD\", \"INTC\", \"QCOM\", \"TXN\", \"MU\", \"AMAT\", \"LRCX\",\n",
    "            \"LLY\", \"BMY\", \"GILD\", \"AMGN\", \"REGN\", \"VRTX\", \"ISRG\",\n",
    "            \"MCD\", \"SBUX\", \"NKE\", \"TGT\", \"LOW\", \"TJX\", \"ORLY\",\n",
    "            \"CAT\", \"DE\", \"HON\", \"UPS\", \"RTX\", \"LMT\", \"GE\",\n",
    "            \"COP\", \"SLB\", \"EOG\", \"PXD\", \"MPC\", \"VLO\", \"PSX\"\n",
    "        ]\n",
    "    },\n",
    "    \"E\": {\n",
    "        \"name\": \"FX Majors\",\n",
    "        \"tickers\": [\n",
    "            \"EURUSD=X\", \"GBPUSD=X\", \"USDJPY=X\", \"USDCHF=X\",\n",
    "            \"AUDUSD=X\", \"USDCAD=X\", \"NZDUSD=X\",\n",
    "            \"EURGBP=X\", \"EURJPY=X\", \"GBPJPY=X\"\n",
    "        ]\n",
    "    },\n",
    "    \"F\": {\n",
    "        \"name\": \"Oslo Børs\",\n",
    "        \"tickers\": [\n",
    "            \"EQNR.OL\", \"DNB.OL\", \"TEL.OL\", \"MOWI.OL\", \"ORK.OL\",\n",
    "            \"YAR.OL\", \"AKRBP.OL\", \"SALM.OL\", \"SUBC.OL\", \"FRO.OL\",\n",
    "            \"AKER.OL\", \"STB.OL\", \"GOGL.OL\", \"BAKKA.OL\", \"KOG.OL\"\n",
    "        ]\n",
    "    },\n",
    "    \"G\": {\n",
    "        \"name\": \"European Majors\",\n",
    "        \"tickers\": [\n",
    "            \"SAP.DE\", \"SIE.DE\", \"ALV.DE\", \"DTE.DE\", \"BAS.DE\",\n",
    "            \"BMW.DE\", \"MBG.DE\", \"VOW3.DE\", \"ADS.DE\", \"MUV2.DE\",\n",
    "            \"MC.PA\", \"OR.PA\", \"SAN.PA\", \"AI.PA\", \"BNP.PA\",\n",
    "            \"AIR.PA\", \"TTE.PA\", \"SU.PA\", \"CS.PA\", \"CAP.PA\",\n",
    "            \"SHEL.L\", \"AZN.L\", \"HSBA.L\", \"BP.L\", \"GSK.L\",\n",
    "            \"RIO.L\", \"ULVR.L\", \"DGE.L\", \"LLOY.L\", \"VOD.L\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Vis pools\n",
    "print(\"Asset Pools:\")\n",
    "print(\"-\" * 40)\n",
    "total = 0\n",
    "for pool_id, pool in POOLS.items():\n",
    "    count = len(pool['tickers'])\n",
    "    total += count\n",
    "    print(f\"  {pool_id}: {pool['name']} ({count} tickers)\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Total: {total} tickers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Checkpoint System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint():\n",
    "    \"\"\"Last checkpoint fra Google Drive\"\"\"\n",
    "    checkpoint_file = Path(CHECKPOINT_DIR) / \"checkpoint.json\"\n",
    "    if checkpoint_file.exists():\n",
    "        with open(checkpoint_file, 'r') as f:\n",
    "            return json.load(f)\n",
    "    return {\n",
    "        \"completed_tickers\": [],\n",
    "        \"failed_tickers\": [],\n",
    "        \"last_update\": None\n",
    "    }\n",
    "\n",
    "def save_checkpoint(checkpoint):\n",
    "    \"\"\"Lagre checkpoint til Google Drive\"\"\"\n",
    "    checkpoint[\"last_update\"] = datetime.now().isoformat()\n",
    "    checkpoint_file = Path(CHECKPOINT_DIR) / \"checkpoint.json\"\n",
    "    with open(checkpoint_file, 'w') as f:\n",
    "        json.dump(checkpoint, f, indent=2)\n",
    "\n",
    "def save_ticker_data(ticker, df):\n",
    "    \"\"\"Lagre ticker data til CSV\"\"\"\n",
    "    data_dir = Path(CHECKPOINT_DIR) / \"data\"\n",
    "    data_dir.mkdir(exist_ok=True)\n",
    "    safe_ticker = ticker.replace(\"/\", \"_\").replace(\"=\", \"_\")\n",
    "    filepath = data_dir / f\"{safe_ticker}.csv\"\n",
    "    df.to_csv(filepath)\n",
    "    return filepath\n",
    "\n",
    "# Last eksisterende checkpoint\n",
    "checkpoint = load_checkpoint()\n",
    "print(f\"Checkpoint status:\")\n",
    "print(f\"  Fullført: {len(checkpoint['completed_tickers'])}\")\n",
    "print(f\"  Feilet: {len(checkpoint['failed_tickers'])}\")\n",
    "print(f\"  Sist oppdatert: {checkpoint.get('last_update', 'Aldri')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fetch-funksjon med Rate Limiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_with_backoff(ticker, start_date, end_date):\n",
    "    \"\"\"Fetch data med eksponentiell backoff for Colab\"\"\"\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            if attempt > 0:\n",
    "                delay = RETRY_BASE_DELAY * (2 ** (attempt - 1))\n",
    "                print(f\"  Retry {attempt + 1}/{MAX_RETRIES}, venter {delay:.0f}s...\")\n",
    "                time.sleep(delay)\n",
    "\n",
    "            df = yf.download(\n",
    "                ticker,\n",
    "                start=start_date.strftime('%Y-%m-%d'),\n",
    "                end=(end_date + timedelta(days=1)).strftime('%Y-%m-%d'),\n",
    "                interval='1d',\n",
    "                auto_adjust=False,\n",
    "                progress=False,\n",
    "                threads=False\n",
    "            )\n",
    "\n",
    "            if df is None or df.empty:\n",
    "                print(f\"  [{ticker}] Tom respons\")\n",
    "                continue\n",
    "\n",
    "            df = df.dropna(subset=['Close'])\n",
    "            if df.empty:\n",
    "                print(f\"  [{ticker}] Ingen gyldige data etter cleaning\")\n",
    "                continue\n",
    "\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            error_str = str(e).lower()\n",
    "            is_rate_limit = any(x in error_str for x in [\n",
    "                \"too many requests\", \"rate limit\", \"429\", \"throttle\"\n",
    "            ])\n",
    "\n",
    "            if is_rate_limit:\n",
    "                print(f\"  [{ticker}] RATE LIMITED! Venter {RATE_LIMIT_BACKOFF}s...\")\n",
    "                time.sleep(RATE_LIMIT_BACKOFF)\n",
    "            else:\n",
    "                print(f\"  [{ticker}] Feil: {e}\")\n",
    "\n",
    "    return None\n",
    "\n",
    "print(\"Fetch-funksjon klar!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hovedfunksjon - Kjør Backfill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_backfill(pool_ids=None, resume=True):\n",
    "    \"\"\"Kjør backfill for valgte pools\"\"\"\n",
    "    global checkpoint\n",
    "\n",
    "    # Bestem tickers\n",
    "    if pool_ids:\n",
    "        tickers = []\n",
    "        for pid in pool_ids:\n",
    "            if pid in POOLS:\n",
    "                tickers.extend(POOLS[pid]['tickers'])\n",
    "        tickers = list(set(tickers))\n",
    "    else:\n",
    "        tickers = []\n",
    "        for pool in POOLS.values():\n",
    "            tickers.extend(pool['tickers'])\n",
    "        tickers = list(set(tickers))\n",
    "\n",
    "    # Filtrer ut allerede fullførte\n",
    "    if resume:\n",
    "        completed = set(checkpoint['completed_tickers'])\n",
    "        pending = [t for t in tickers if t not in completed]\n",
    "    else:\n",
    "        checkpoint = {\"completed_tickers\": [], \"failed_tickers\": [], \"last_update\": None}\n",
    "        pending = tickers\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"IoS-001 BACKFILL - STIG-2025-001\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Pools: {pool_ids or 'ALL'}\")\n",
    "    print(f\"Total tickers: {len(tickers)}\")\n",
    "    print(f\"Allerede fullført: {len(checkpoint['completed_tickers'])}\")\n",
    "    print(f\"Gjenstår: {len(pending)}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    if not pending:\n",
    "        print(\"Ingen tickers å prosessere!\")\n",
    "        return\n",
    "\n",
    "    # Datoer\n",
    "    end_date = date.today() - timedelta(days=1)\n",
    "    start_date = date.today() - timedelta(days=MAX_HISTORY_YEARS * 365)\n",
    "\n",
    "    results = {\n",
    "        \"processed\": 0,\n",
    "        \"success\": 0,\n",
    "        \"failed\": 0,\n",
    "        \"total_rows\": 0\n",
    "    }\n",
    "\n",
    "    batch_count = 0\n",
    "\n",
    "    for i, ticker in enumerate(tqdm(pending, desc=\"Backfill\")):\n",
    "        results[\"processed\"] += 1\n",
    "\n",
    "        try:\n",
    "            print(f\"\\n[{i+1}/{len(pending)}] {ticker}...\")\n",
    "\n",
    "            df = fetch_with_backoff(ticker, start_date, end_date)\n",
    "\n",
    "            if df is None or df.empty:\n",
    "                print(f\"  FEILET: Ingen data\")\n",
    "                results[\"failed\"] += 1\n",
    "                if ticker not in checkpoint[\"failed_tickers\"]:\n",
    "                    checkpoint[\"failed_tickers\"].append(ticker)\n",
    "            else:\n",
    "                # Lagre til CSV\n",
    "                csv_path = save_ticker_data(ticker, df)\n",
    "                rows = len(df)\n",
    "                results[\"total_rows\"] += rows\n",
    "\n",
    "                # Iron Curtain status\n",
    "                is_crypto = ticker.endswith(\"-USD\")\n",
    "                if is_crypto:\n",
    "                    quarantine, full = CRYPTO_QUARANTINE, CRYPTO_FULL_HISTORY\n",
    "                else:\n",
    "                    quarantine, full = EQUITY_FX_QUARANTINE, EQUITY_FX_FULL_HISTORY\n",
    "\n",
    "                if rows < quarantine:\n",
    "                    status = \"QUARANTINED\"\n",
    "                elif rows < full:\n",
    "                    status = \"SHORT_HISTORY\"\n",
    "                else:\n",
    "                    status = \"FULL_HISTORY\"\n",
    "\n",
    "                print(f\"  OK: {rows} rader, {status}\")\n",
    "                results[\"success\"] += 1\n",
    "\n",
    "                if ticker not in checkpoint[\"completed_tickers\"]:\n",
    "                    checkpoint[\"completed_tickers\"].append(ticker)\n",
    "                if ticker in checkpoint[\"failed_tickers\"]:\n",
    "                    checkpoint[\"failed_tickers\"].remove(ticker)\n",
    "\n",
    "            # Lagre checkpoint\n",
    "            save_checkpoint(checkpoint)\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\nAvbrutt! Checkpoint lagret.\")\n",
    "            save_checkpoint(checkpoint)\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR: {e}\")\n",
    "            results[\"failed\"] += 1\n",
    "\n",
    "        # Rate limiting\n",
    "        batch_count += 1\n",
    "        time.sleep(DELAY_BETWEEN_ASSETS)\n",
    "\n",
    "        if batch_count >= BATCH_SIZE:\n",
    "            batch_count = 0\n",
    "            print(f\"\\nBatch pause: {DELAY_BETWEEN_BATCHES}s...\")\n",
    "            time.sleep(DELAY_BETWEEN_BATCHES)\n",
    "\n",
    "    # Oppsummering\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"FULLFØRT\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Prosessert: {results['processed']}\")\n",
    "    print(f\"Suksess: {results['success']}\")\n",
    "    print(f\"Feilet: {results['failed']}\")\n",
    "    print(f\"Totalt rader: {results['total_rows']}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Kjør Pool A (ETFs) - START HER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kjør Pool A\n",
    "results_a = run_backfill(pool_ids=[\"A\"], resume=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Kjør Pool B (Mag7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kjør Pool B\n",
    "results_b = run_backfill(pool_ids=[\"B\"], resume=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Kjør Pool C (Crypto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kjør Pool C\n",
    "results_c = run_backfill(pool_ids=[\"C\"], resume=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Kjør Alle Gjenværende Pools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kjør D, E, F, G\n",
    "results_rest = run_backfill(pool_ids=[\"D\", \"E\", \"F\", \"G\"], resume=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Generer Import Script for Lokal Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generer Python import script\n",
    "data_dir = Path(CHECKPOINT_DIR) / \"data\"\n",
    "csv_files = list(data_dir.glob(\"*.csv\"))\n",
    "\n",
    "print(f\"Fant {len(csv_files)} CSV-filer\")\n",
    "\n",
    "import_script = f'''\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "IoS-001 IMPORT SCRIPT\n",
    "Generert: {datetime.now().isoformat()}\n",
    "Filer: {len(csv_files)}\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Database config\n",
    "DB_CONFIG = {{\n",
    "    \"host\": \"127.0.0.1\",\n",
    "    \"port\": \"54322\",\n",
    "    \"database\": \"postgres\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"postgres\"\n",
    "}}\n",
    "\n",
    "DATA_DIR = Path(\"./ios001_data\")  # Kopier CSV-filene hit\n",
    "\n",
    "def import_csv(conn, filepath):\n",
    "    ticker = filepath.stem.replace(\"_\", \"-\").replace(\"-X\", \"=X\")\n",
    "    df = pd.read_csv(filepath, index_col=0, parse_dates=True)\n",
    "\n",
    "    insert_sql = \"\"\"\n",
    "        INSERT INTO fhq_data.price_series (\n",
    "            listing_id, timestamp, vendor_id, frequency, price_type,\n",
    "            open, high, low, close, volume, adj_close,\n",
    "            source_id, is_verified\n",
    "        ) VALUES %s\n",
    "        ON CONFLICT (listing_id, timestamp, vendor_id) DO UPDATE SET\n",
    "            open = EXCLUDED.open,\n",
    "            high = EXCLUDED.high,\n",
    "            low = EXCLUDED.low,\n",
    "            close = EXCLUDED.close,\n",
    "            volume = EXCLUDED.volume,\n",
    "            adj_close = EXCLUDED.adj_close\n",
    "    \"\"\"\n",
    "\n",
    "    values = []\n",
    "    for idx, row in df.iterrows():\n",
    "        ts = idx.tz_localize(None) if hasattr(idx, \"tz_localize\") and idx.tzinfo else idx\n",
    "        values.append((\n",
    "            ticker, ts, \"YFINANCE_COLAB\", \"DAILY\", \"RAW\",\n",
    "            row.get(\"Open\"), row.get(\"High\"), row.get(\"Low\"),\n",
    "            row.get(\"Close\"), row.get(\"Volume\"),\n",
    "            row.get(\"Adj Close\", row.get(\"Close\")),\n",
    "            \"COLAB_IMPORT\", False\n",
    "        ))\n",
    "\n",
    "    with conn.cursor() as cur:\n",
    "        execute_values(cur, insert_sql, values)\n",
    "    conn.commit()\n",
    "    return len(values)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    conn = psycopg2.connect(**DB_CONFIG)\n",
    "    csv_files = list(DATA_DIR.glob(\"*.csv\"))\n",
    "    print(f\"Importerer {{len(csv_files)}} filer...\")\n",
    "\n",
    "    total = 0\n",
    "    for f in csv_files:\n",
    "        try:\n",
    "            rows = import_csv(conn, f)\n",
    "            total += rows\n",
    "            print(f\"  {{f.stem}}: {{rows}} rader\")\n",
    "        except Exception as e:\n",
    "            print(f\"  {{f.stem}}: FEIL - {{e}}\")\n",
    "\n",
    "    conn.close()\n",
    "    print(f\"Totalt: {{total}} rader importert\")\n",
    "'''\n",
    "\n",
    "import_script_path = Path(CHECKPOINT_DIR) / \"import_to_db.py\"\n",
    "with open(import_script_path, 'w') as f:\n",
    "    f.write(import_script)\n",
    "\n",
    "print(f\"Import script generert: {import_script_path}\")\n",
    "print(\"\\nInstruksjoner:\")\n",
    "print(\"1. Last ned CSV-filene fra Google Drive\")\n",
    "print(\"2. Kopier til ./ios001_data/\")\n",
    "print(\"3. Kjør: python import_to_db.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Sjekk Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oppdater checkpoint\n",
    "checkpoint = load_checkpoint()\n",
    "\n",
    "# Analyser data\n",
    "data_dir = Path(CHECKPOINT_DIR) / \"data\"\n",
    "csv_files = list(data_dir.glob(\"*.csv\"))\n",
    "\n",
    "status_counts = {\"FULL_HISTORY\": 0, \"SHORT_HISTORY\": 0, \"QUARANTINED\": 0}\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    df = pd.read_csv(csv_file)\n",
    "    rows = len(df)\n",
    "    ticker = csv_file.stem\n",
    "\n",
    "    is_crypto = \"-USD\" in ticker\n",
    "    if is_crypto:\n",
    "        quarantine, full = CRYPTO_QUARANTINE, CRYPTO_FULL_HISTORY\n",
    "    else:\n",
    "        quarantine, full = EQUITY_FX_QUARANTINE, EQUITY_FX_FULL_HISTORY\n",
    "\n",
    "    if rows < quarantine:\n",
    "        status_counts[\"QUARANTINED\"] += 1\n",
    "    elif rows < full:\n",
    "        status_counts[\"SHORT_HISTORY\"] += 1\n",
    "    else:\n",
    "        status_counts[\"FULL_HISTORY\"] += 1\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"IoS-001 IRON CURTAIN STATUS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Fullført tickers: {len(checkpoint['completed_tickers'])}\")\n",
    "print(f\"Feilet tickers: {len(checkpoint['failed_tickers'])}\")\n",
    "print(f\"CSV-filer: {len(csv_files)}\")\n",
    "print()\n",
    "print(\"Data Quality Status:\")\n",
    "print(f\"  FULL_HISTORY (5+ år): {status_counts['FULL_HISTORY']}\")\n",
    "print(f\"  SHORT_HISTORY (1-5 år): {status_counts['SHORT_HISTORY']}\")\n",
    "print(f\"  QUARANTINED (<1 år): {status_counts['QUARANTINED']}\")\n",
    "print()\n",
    "print(f\"IoS-003 Eligible: {status_counts['FULL_HISTORY'] + status_counts['SHORT_HISTORY']}\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
